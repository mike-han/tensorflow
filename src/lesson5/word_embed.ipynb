{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e992935-3f20-492e-9a00-46faa6a4388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eecc2e8-2416-47fc-a22f-9706bfc6b868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n",
    "train_data, test_data = imdb['train'], imdb['test']\n",
    "\n",
    "traning_sentences = []\n",
    "training_labels = []\n",
    "\n",
    "testing_sequences = []\n",
    "testing_labels = []\n",
    "\n",
    "for s,l in train_data:\n",
    "  traning_sentences.append(str(s.numpy()))\n",
    "  training_labels.append(l.numpy())\n",
    "\n",
    "for s,l in test_data:\n",
    "  testing_sequences.append(str(s.numpy()))\n",
    "  testing_labels.append(l.numpy())\n",
    "\n",
    "training_labels_final = np.array(training_labels)\n",
    "testing_labels_final = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2113baa-79ae-4684-98e6-71810e08cf17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b\\'If you love Japanese monster movies, you\\\\\\'ll love this action packed battle pitting an alien invader, intent upon conquering the Earth, and a \"Giant Robot\" with an armory of super weaponry. The alien, \"Emporer Guillotine,\" from the planet Gargoyle, has a army of thugs called, (of course) \"the gargoyle gang,\" as well as an endless supply of immense hostile creatures that are routinely loosed upon the Earth to smash buildings, make loud noises, panic the populace, etc. A little kid, named Johnny Sokko, has the Giant Robot at his beckon call, and sends the Robot, as needed, to beat up, and then blast these creatures. Johnny joins a group of \"good spies\" called Unicorn, and endeavors to help save the world.<br /><br />In spite of the campy nature, unintentionally humorous dialog, and the fact that the target audience was obviously children, this movie has non-stop action, colorful characters, decent special effects, and just happens to be downright fun to watch. Battle scenes are well executed, and frequent, as the storyline requires. The good guys and bad guys both made sure they had an inexhaustible supply of bombs, lasers, ammunition, and schemes to attack each other. In spite of the fact the movie was constructed from edited episodes of a TV series, the plot actually develops, and reaches an ultimate conclusion.<br /><br />The film has a positive outlook and appeals to everyone\\\\\\'s (especially kids\\\\\\') desire to destroy evil in its many forms. Kids may be the target audience, but it\\\\\\'s fun for everyone to laugh at its comical silliness; yet, at the same time, root for the good guys to prevail and \"save the world.\" The acting is cheesy in places, but that is the charm: there are several lines of corny dialog (possibly translation errors or possibly intentional jokes by the movie makers), and you\\\\\\'ll find yourself quoting these absurd lines later.<br /><br />Admittedly, this film is not high in production quality or budget. However, for what it is, campy sci-fi, it\\\\\\'s enjoyable for some laughs. I recommend it to anyone with a sense of humor for that sort of thing.\\''"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_sequences[1]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6fa54a66-fcdf-4bad-9fbb-1350c65327db",
   "metadata": {},
   "source": [
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 120\n",
    "trunc_type = 'post'\n",
    "oov_token = \"<OOV>\"\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
    "tokenizer.fit_on_texts(traning_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(traning_sentences)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(testing_sequences)\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
    "# model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "model.add(tf.keras.layers.Dense(6, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(padded, training_labels_final, epochs=4, validation_data=(test_padded, testing_labels_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ba9ef0-94bd-4a62-9176-bf8c20d2b3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "weights = model.layers[0].get_weights()[0]\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for word_num in range(1, vocab_size):\n",
    "    word = reverse_word_index[word_num]\n",
    "    embeddings = weights[word_num]\n",
    "    out_m.write(word + '\\n')\n",
    "    out_v.write('\\t'.join([str(x) for x in embeddings]) + '\\n')\n",
    "    \n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f65dbffd-257d-4ad5-a70a-a45472c5d43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.22357793]]\n",
      "[[0.73812497]]\n"
     ]
    }
   ],
   "source": [
    "def convert_text_to_padded_sequences(text):\n",
    "    sequences = tokenizer.texts_to_sequences([text])\n",
    "    padded = pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)\n",
    "    return padded\n",
    "\n",
    "test_data1 = convert_text_to_padded_sequences('This movie is so boring')\n",
    "predict1 = model.predict(test_data1)\n",
    "test_data2 = convert_text_to_padded_sequences('The film is wonderful')\n",
    "predict2 = model.predict(test_data2)\n",
    "print(predict1)\n",
    "print(predict2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
