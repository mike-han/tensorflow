{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-31 14:29:13.377510: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn import preprocessing\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "max_len = 384\n",
    "configuration = BertConfig()\n",
    "# download data [here](https://www.kaggle.com/datasets/namanj27/ner-dataset)\n",
    "data_csv = \"ner_dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the slow pretrained tokenizer\n",
    "slow_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "save_path = \"bert_base_uncased/\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "slow_tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fast tokenizer from saved file\n",
    "tokenizer = BertWordPieceTokenizer(\"bert_base_uncased/vocab.txt\", lowercase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model\n",
    "\n",
    "#### Model\n",
    "\n",
    "Add a fully connected layer that takes token embeddings from BERT as input and\n",
    "predicts probability of that token belonging to each of the possible tags.\n",
    "The fully connected layer in the code below shows a `keras.layers.Dense` layer\n",
    "with `num_tags+1` units to accomodate a padding label.\n",
    "\n",
    "#### Masked Loss\n",
    "\n",
    "Each batch of data will consist of variable sized sentence tokens with\n",
    "appropriate padding in both input and target.\n",
    "During loss calculation, we ignore the loss corresponding padding tokens\n",
    "in the target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=False, reduction=tf.keras.losses.Reduction.NONE\n",
    ")\n",
    "\n",
    "def masked_ce_loss(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 17))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "def create_model(num_tags):\n",
    "    ## BERT encoder\n",
    "    encoder = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    ## NER Model\n",
    "    input_ids = keras.layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "    token_type_ids = keras.layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "    attention_mask = keras.layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "    \n",
    "    outputs = encoder(\n",
    "        input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n",
    "    )\n",
    "    embedding = outputs[0]\n",
    "    embedding = keras.layers.Dropout(0.3)(embedding)\n",
    "    tag_logits = keras.layers.Dense(num_tags+1, activation='softmax')(embedding)\n",
    "    \n",
    "    model = keras.Model(\n",
    "        inputs=[input_ids, token_type_ids, attention_mask],\n",
    "        outputs=[tag_logits],\n",
    "    )\n",
    "    optimizer = keras.optimizers.Adam(lr=3e-5)\n",
    "    model.compile(optimizer=optimizer, loss=masked_ce_loss, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess dataset\n",
    "\n",
    "First, we read the convert the rows of our data file into sentences and lists of\n",
    "tags. `sklearn.preprocessing.LabelEncoder` encodes each tag in a number.\n",
    "\n",
    "Then, we create tokenize each sentence using BERT tokenizer from huggingface.\n",
    "\n",
    "After tokenization each sentence is represented by a set of input_ids,\n",
    "attention_masks and token_type_ids. Note that a single word may be tokenized into\n",
    "multiple tokens. In that case, each token gets the label of the original word.\n",
    "eg.\n",
    "\n",
    "#### DataFrame\n",
    "\n",
    "`pd.DataFrame.fillna(method=\"ffill\")` can fill `NaN` with the forward word.\n",
    "\n",
    "For example:\n",
    "\n",
    "df:\n",
    "\n",
    "```\n",
    "\tSentence #\tWord\tPOS\tTag\n",
    "0\tSentence: 1\tThousands\tNNS\tO\n",
    "1\tNaN\tof\tIN\tO\n",
    "2\tNaN\tdemonstrators\tNNS\tO\n",
    "3\tNaN\thave\tVBP\tO\n",
    "```\n",
    "\n",
    "df[\"Sentence #\"].fillna(method=\"ffill\"):\n",
    "\n",
    "```\n",
    "\tSentence #\tWord\tPOS\tTag\n",
    "0\tSentence: 1\tThousands\tNNS\tO\n",
    "1\tSentence: 1\tof\tIN\tO\n",
    "2\tSentence: 1\tdemonstrators\tNNS\tO\n",
    "3\tSentence: 1\thave\tVBP\tO\n",
    "```\n",
    "\n",
    "#### LabelEncoder\n",
    "\n",
    "Encode target labels with value between 0 and n_classes-1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv(data_path):\n",
    "    df = pd.read_csv(data_path, encoding=\"latin-1\")\n",
    "    df.loc[:, \"Sentence #\"] = df[\"Sentence #\"].fillna(method=\"ffill\")\n",
    "    enc_tag = preprocessing.LabelEncoder()\n",
    "    df.loc[:, \"Tag\"] = enc_tag.fit_transform(df[\"Tag\"])\n",
    "    sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
    "    tag = df.groupby(\"Sentence #\")[\"Tag\"].apply(list).values\n",
    "    return sentences, tag, enc_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inputs_targets(data_csv):\n",
    "    dataset_dict = {\n",
    "        \"input_ids\": [],\n",
    "        \"token_type_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"tags\": []\n",
    "    }\n",
    "    sentences, tags, tag_encoder = process_csv(data_csv)\n",
    "    \n",
    "    for sentence, tag in zip(sentences, tags):\n",
    "        input_ids = []\n",
    "        target_tags = []\n",
    "        for idx, word in enumerate(sentence):\n",
    "            ids = tokenizer.encode(word, add_special_tokens=False)\n",
    "            input_ids.extend(ids.ids)\n",
    "            num_tokens = len(ids)\n",
    "            target_tags.extend([tag[idx]] * num_tokens)\n",
    "        \n",
    "        \n",
    "        # Pad truncate\n",
    "        input_ids = input_ids[:max_len - 2]\n",
    "        target_tags = target_tags[:max_len - 2]\n",
    "\n",
    "        input_ids = [101] + input_ids + [102]\n",
    "        target_tags = [16] + target_tags + [16] # why [16]?\n",
    "        token_type_ids = [0] * len(input_ids)\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        padding_len = max_len - len(input_ids)\n",
    "\n",
    "        input_ids = input_ids + ([0] * padding_len)\n",
    "        attention_mask = attention_mask + ([0] * padding_len)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_len)\n",
    "        target_tags = target_tags + ([17] * padding_len)\n",
    "        \n",
    "        dataset_dict[\"input_ids\"].append(input_ids)\n",
    "        dataset_dict[\"token_type_ids\"].append(token_type_ids)\n",
    "        dataset_dict[\"attention_mask\"].append(attention_mask)\n",
    "        dataset_dict[\"tags\"].append(target_tags)\n",
    "        assert len(target_tags) == max_len, f'{len(input_ids)}, {len(target_tags)}'\n",
    "        \n",
    "    for key in dataset_dict:\n",
    "        dataset_dict[key] = np.array(dataset_dict[key])\n",
    "\n",
    "    x = [\n",
    "        dataset_dict[\"input_ids\"],\n",
    "        dataset_dict[\"token_type_ids\"],\n",
    "        dataset_dict[\"attention_mask\"],\n",
    "    ]\n",
    "    y = dataset_dict[\"tags\"]\n",
    "    return x, y, tag_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model\n",
    "\n",
    "Use TPU if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tags = pd.read_csv(data_csv, encoding=\"latin-1\")[\"Tag\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3652c726ae164ed49dcb7be794d0db62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/511M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 384)]        0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 384)]        0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 384)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
      "                                thPoolingAndCrossAt               'input_3[0][0]',                \n",
      "                                tentions(last_hidde               'input_2[0][0]']                \n",
      "                                n_state=(None, 384,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 384, 768)     0           ['tf_bert_model[0][0]']          \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 384, 18)      13842       ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,496,082\n",
      "Trainable params: 109,496,082\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ming8525/miniforge3/envs/tensorflow/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "num_tags = pd.read_csv(data_csv, encoding=\"latin-1\")[\"Tag\"].nunique()\n",
    "\n",
    "use_tpu = None\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    use_tpu = True\n",
    "except:\n",
    "    use_tpu = False\n",
    "\n",
    "if use_tpu:\n",
    "    # Create distribution strategy\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "\n",
    "    # Create model\n",
    "    with strategy.scope():\n",
    "        model = create_model(num_tags)\n",
    "else:\n",
    "    model = create_model(num_tags)\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, tag_encoder = create_inputs_targets(data_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bs = 64 if use_tpu else 16\n",
    "\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=1,\n",
    "    verbose=1,\n",
    "    batch_size=bs,\n",
    "    validation_split=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Check the predicted tags for a sample sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_input_from_text(texts, use_tensor=False):\n",
    "    dataset_dict = {\n",
    "        \"input_ids\": [],\n",
    "        \"token_type_ids\": [],\n",
    "        \"attention_mask\": []\n",
    "    }\n",
    "    for sentence in texts:\n",
    "        input_ids = []\n",
    "        for idx, word in enumerate(sentence.split()):\n",
    "            ids = tokenizer.encode(word, add_special_tokens=False)\n",
    "            input_ids.extend(ids.ids)\n",
    "\n",
    "        # Pad and create attention masks.\n",
    "        # Skip if truncation is needed\n",
    "        input_ids = input_ids[:max_len - 2]\n",
    "\n",
    "        input_ids = [101] + input_ids + [102]\n",
    "        n_tokens = len(input_ids)\n",
    "        token_type_ids = [0] * len(input_ids)\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        padding_len = max_len - len(input_ids)\n",
    "\n",
    "        input_ids = input_ids + ([0] * padding_len)\n",
    "        attention_mask = attention_mask + ([0] * padding_len)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_len)\n",
    "\n",
    "        dataset_dict[\"input_ids\"].append(input_ids)\n",
    "        dataset_dict[\"token_type_ids\"].append(token_type_ids)\n",
    "        dataset_dict[\"attention_mask\"].append(attention_mask)\n",
    "\n",
    "    for key in dataset_dict:\n",
    "        dataset_dict[key] = tf.constant(dataset_dict[key]) if use_tensor else np.array(dataset_dict[key])\n",
    "\n",
    "    x = [\n",
    "        dataset_dict[\"input_ids\"],\n",
    "        dataset_dict[\"token_type_ids\"],\n",
    "        dataset_dict[\"attention_mask\"],\n",
    "    ]\n",
    "    return x, n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_text(text, model):\n",
    "    x_test, n_tokens = create_test_input_from_text([text], use_tensor=True)\n",
    "    # pred_test = model(x_test)\n",
    "    pred_test = model.predict(x_test) if hasattr(model, 'predict') else model(x_test)\n",
    "    # ignore predictions of padding tokens\n",
    "    pred_tags = np.argmax(pred_test, 2)[0][:n_tokens]\n",
    "    le_dict = dict(zip(tag_encoder.transform(\n",
    "        tag_encoder.classes_), tag_encoder.classes_))\n",
    "    tags = [le_dict.get(_, '[pad]') for _ in pred_tags]\n",
    "    res = []\n",
    "    words = {\n",
    "        'word': '',\n",
    "        'tag': None\n",
    "    }\n",
    "    for idx, tag in enumerate(tags):\n",
    "        token = x_test[0][0][idx]\n",
    "        token = token.numpy()\n",
    "        if(token == 101 or token == 102 or token == None):\n",
    "            continue\n",
    "        if(tag != 'O'):\n",
    "            pre, suf = tag.split('-')\n",
    "            words['tag'] = suf\n",
    "            word = tokenizer.decode([token])\n",
    "            words['word'] =  words['word'] + word if words['word'] else word\n",
    "        else:\n",
    "            if(words['tag']):\n",
    "                res.append(words)\n",
    "            words = {\n",
    "                'word': '',\n",
    "                'tag': None\n",
    "            }\n",
    "    return pd.DataFrame(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'ner'\n",
    "saved_model_path = './{}_bert'.format(dataset_name.replace('/', '_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saved_model_path, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_model = tf.saved_model.load(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = \"alex like traval in china\"\n",
    "print(predict_from_text(test_inputs, reloaded_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8639f22e1530ee2a5f2548bc0245ffed892f59880c37de02f243da87cdc45584"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
