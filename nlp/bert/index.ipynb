{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "from official.modeling import tf_utils\n",
    "from official import nlp\n",
    "from official.nlp import bert\n",
    "\n",
    "# Load the required submodules\n",
    "import official.nlp.optimization\n",
    "import official.nlp.bert.bert_models\n",
    "import official.nlp.bert.configs\n",
    "import official.nlp.bert.run_classifier\n",
    "import official.nlp.bert.tokenization\n",
    "import official.nlp.data.classifier_data_lib\n",
    "import official.nlp.modeling.losses\n",
    "import official.nlp.modeling.models\n",
    "import official.nlp.modeling.networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_folder_bert = \"gs://cloud-tpu-checkpoints/bert/v3/uncased_L-12_H-768_A-12\"\n",
    "tf.io.gfile.listdir(gs_folder_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue, info = tfds.load('glue/mrpc', with_info=True,\n",
    "                       # It's small, load the whole dataset\n",
    "                       batch_size=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(glue.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_train = glue['train']\n",
    "\n",
    "for key, value in glue_train.items():\n",
    "  print(f\"{key:9s}: {value[0].numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up tokenizer to generate Tensorflow dataset\n",
    "tokenizer = bert.tokenization.FullTokenizer(\n",
    "  vocab_file=os.path.join(gs_folder_bert, 'vocab.txt'), do_lower_case=True)\n",
    "\n",
    "print(\"Vocab size: \", len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(s, tokenizer):\n",
    "   tokens = list(tokenizer.tokenize(s))\n",
    "   tokens.append('[SEP]')\n",
    "   return tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "def bert_encode(glue_dict, tokenizer):\n",
    "  num_examples = len(glue_dict[\"sentence1\"])\n",
    "\n",
    "  sentence1 = tf.ragged.constant([\n",
    "      encode_sentence(s, tokenizer)\n",
    "      for s in np.array(glue_dict[\"sentence1\"])])\n",
    "  sentence2 = tf.ragged.constant([\n",
    "      encode_sentence(s, tokenizer)\n",
    "       for s in np.array(glue_dict[\"sentence2\"])])\n",
    "\n",
    "  cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0]\n",
    "  input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\n",
    "\n",
    "  input_mask = tf.ones_like(input_word_ids).to_tensor()\n",
    "\n",
    "  type_cls = tf.zeros_like(cls)\n",
    "  type_s1 = tf.zeros_like(sentence1)\n",
    "  type_s2 = tf.ones_like(sentence2)\n",
    "  input_type_ids = tf.concat(\n",
    "      [type_cls, type_s1, type_s2], axis=-1).to_tensor()\n",
    "\n",
    "  inputs = {\n",
    "      'input_word_ids': input_word_ids.to_tensor(),\n",
    "      'input_mask': input_mask,\n",
    "      'input_type_ids': input_type_ids}\n",
    "\n",
    "  return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_train = bert_encode(glue['train'], tokenizer)\n",
    "glue_train_labels = glue['train']['label']\n",
    "\n",
    "glue_validation = bert_encode(glue['validation'], tokenizer)\n",
    "glue_validation_labels = glue['validation']['label']\n",
    "\n",
    "glue_test = bert_encode(glue['test'], tokenizer)\n",
    "glue_test_labels  = glue['test']['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in glue_train.items():\n",
    "  print(f'{key:15s} shape: {value.shape}')\n",
    "\n",
    "print(f'glue_train_labels shape: {glue_train_labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "bert_config_file = os.path.join(gs_folder_bert, \"bert_config.json\")\n",
    "config_dict = json.loads(tf.io.gfile.GFile(bert_config_file).read())\n",
    "\n",
    "bert_config = bert.configs.BertConfig.from_dict(config_dict)\n",
    "\n",
    "config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_classifier, bert_encoder = bert.bert_models.classifier_model(\n",
    "    bert_config, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(bert_classifier, show_shapes=True, dpi=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8639f22e1530ee2a5f2548bc0245ffed892f59880c37de02f243da87cdc45584"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
