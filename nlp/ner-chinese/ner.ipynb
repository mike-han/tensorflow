{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn import preprocessing\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-chinese\"\n",
    "max_len = 384\n",
    "configuration = BertConfig()\n",
    "train_data_path = \"dataset/msra_train_bio\"\n",
    "test_data_path = \"dataset/msra_test_bio\"\n",
    "tags_data_path = \"dataset/tags.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(filename):\n",
    "    with open(filename) as f:\n",
    "        sequences = {\n",
    "            'sentence': [],\n",
    "            'word': [],\n",
    "            'tag': []\n",
    "        }\n",
    "        sequnce_index = 0\n",
    "        for line in f:\n",
    "            if line and line != '\\n':\n",
    "                sequences['sentence'].append(str(sequnce_index))\n",
    "                word, tag = line.strip().split('\\t')\n",
    "                sequences['word'].append(word)\n",
    "                sequences['tag'].append(tag)\n",
    "            else:\n",
    "                sequnce_index += 1\n",
    "\n",
    "    return sequences\n",
    "\n",
    "def process_data(path):\n",
    "    data = preprocess_data(path)\n",
    "    df = pd.DataFrame(data)\n",
    "    enc_tag = preprocessing.LabelEncoder()\n",
    "    df.loc[:, 'tag'] = enc_tag.fit_transform(df['tag'])\n",
    "    sentences = df.groupby('sentence')[\"word\"].apply(list).values\n",
    "    tag = df.groupby('sentence')['tag'].apply(list).values\n",
    "    return sentences, tag, enc_tag\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inputs_targets(path):\n",
    "    dataset_dict = {\n",
    "        \"input_ids\": [],\n",
    "        \"token_type_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"tags\": []\n",
    "    }\n",
    "    sentences, tags, tag_encoder = process_data(path)\n",
    "    \n",
    "    for sentence, tag in zip(sentences, tags):\n",
    "        input_ids = []\n",
    "        target_tags = []\n",
    "        for idx, word in enumerate(sentence):\n",
    "            ids = tokenizer.encode(word, add_special_tokens=False)\n",
    "            input_ids.extend(ids.ids)\n",
    "            num_tokens = len(ids)\n",
    "            target_tags.extend([tag[idx]] * num_tokens)\n",
    "        \n",
    "        \n",
    "        # Pad truncate\n",
    "        input_ids = input_ids[:max_len - 2]\n",
    "        target_tags = target_tags[:max_len - 2]\n",
    "\n",
    "        input_ids = [101] + input_ids + [102]\n",
    "        target_tags = [16] + target_tags + [16] # why [16]?\n",
    "        token_type_ids = [0] * len(input_ids)\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        padding_len = max_len - len(input_ids)\n",
    "\n",
    "        input_ids = input_ids + ([0] * padding_len)\n",
    "        attention_mask = attention_mask + ([0] * padding_len)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_len)\n",
    "        target_tags = target_tags + ([17] * padding_len)\n",
    "        \n",
    "        dataset_dict[\"input_ids\"].append(input_ids)\n",
    "        dataset_dict[\"token_type_ids\"].append(token_type_ids)\n",
    "        dataset_dict[\"attention_mask\"].append(attention_mask)\n",
    "        dataset_dict[\"tags\"].append(target_tags)\n",
    "        assert len(target_tags) == max_len, f'{len(input_ids)}, {len(target_tags)}'\n",
    "        \n",
    "    for key in dataset_dict:\n",
    "        dataset_dict[key] = np.array(dataset_dict[key])\n",
    "\n",
    "    x = [\n",
    "        dataset_dict[\"input_ids\"],\n",
    "        dataset_dict[\"token_type_ids\"],\n",
    "        dataset_dict[\"attention_mask\"],\n",
    "    ]\n",
    "    y = dataset_dict[\"tags\"]\n",
    "    return x, y, tag_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags = pd.read_csv(tags_data_path, sep='\\t', names=[\"Tags\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the slow pretrained tokenizer\n",
    "slow_tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "save_path = \"{}_tokens/\".format(model_name)\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "slow_tokenizer.save_pretrained(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fast tokenizer from saved file\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    \"{}_tokens/vocab.txt\".format(model_name), lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=False, reduction=tf.keras.losses.Reduction.NONE\n",
    ")\n",
    "\n",
    "\n",
    "def masked_ce_loss(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 17))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "\n",
    "def create_model(num_tags):\n",
    "    # BERT encoder\n",
    "    encoder = TFBertModel.from_pretrained(model_name)\n",
    "\n",
    "    # NER Model\n",
    "    input_ids = keras.layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "    token_type_ids = keras.layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "    attention_mask = keras.layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "\n",
    "    outputs = encoder(\n",
    "        input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n",
    "    )\n",
    "    embedding = outputs[0]\n",
    "    embedding = keras.layers.Dropout(0.3)(embedding)\n",
    "    tag_logits = keras.layers.Dense(\n",
    "        num_tags+1, activation='softmax')(embedding)\n",
    "\n",
    "    model = keras.Model(\n",
    "        inputs=[input_ids, token_type_ids, attention_mask],\n",
    "        outputs=[tag_logits],\n",
    "    )\n",
    "    optimizer = keras.optimizers.Adam(lr=3e-5)\n",
    "    model.compile(optimizer=optimizer, loss=masked_ce_loss,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tags = len(all_tags)\n",
    "\n",
    "use_tpu = None\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    use_tpu = True\n",
    "except:\n",
    "    use_tpu = False\n",
    "\n",
    "if use_tpu:\n",
    "    # Create distribution strategy\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "\n",
    "    # Create model\n",
    "    with strategy.scope():\n",
    "        model = create_model(num_tags)\n",
    "else:\n",
    "    model = create_model(num_tags)\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, tag_encoder = create_inputs_targets(train_data_path)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a3947d2c3025ae9c68eb4cc6fd7e30dcf84ba15c0321b0f6c3f08337a692581c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('tensor')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
