{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-31 14:50:45.604995: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn import preprocessing\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-chinese\" # https://huggingface.co/bert-base-chinese\n",
    "max_len = 384\n",
    "# download from [here](https://github.com/lemonhu/NER-BERT-pytorch/tree/master/data/msra)\n",
    "train_data_path = \"dataset/msra_train_bio\"\n",
    "test_data_path = \"dataset/msra_test_bio\"\n",
    "tags_data_path = \"dataset/tags.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(filename, max_data_len):\n",
    "    with open(filename) as f:\n",
    "        sequences = {\n",
    "            'sentence': [],\n",
    "            'word': [],\n",
    "            'tag': []\n",
    "        }\n",
    "        sequnce_index = 0\n",
    "        for line in f:\n",
    "            if (max_data_len and len(sequences['word']) > max_data_len):\n",
    "                break\n",
    "            if line and line != '\\n':\n",
    "                sequences['sentence'].append(str(sequnce_index))\n",
    "                splited = line.strip().split('\\t')\n",
    "                word, tag = splited\n",
    "                sequences['word'].append(word)\n",
    "                sequences['tag'].append(tag)\n",
    "            else:\n",
    "                sequnce_index += 1\n",
    "\n",
    "    return sequences\n",
    "\n",
    "\n",
    "def process_data(path, max_data_len):\n",
    "    data = preprocess_data(path, max_data_len)\n",
    "    df = pd.DataFrame(data)\n",
    "    enc_tag = preprocessing.LabelEncoder()\n",
    "    df.loc[:, 'tag'] = enc_tag.fit_transform(df['tag'])\n",
    "    sentences = df.groupby('sentence')[\"word\"].apply(list).values\n",
    "    tag = df.groupby('sentence')['tag'].apply(list).values\n",
    "    return sentences, tag, enc_tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sentence word    tag\n",
      "0          0    中  B-ORG\n",
      "1          0    共  I-ORG\n",
      "2          0    中  I-ORG\n",
      "3          0    央  I-ORG\n",
      "4          0    致      O\n",
      "..       ...  ...    ...\n",
      "151        1    基      O\n",
      "152        1    本      O\n",
      "153        1    任      O\n",
      "154        1    务      O\n",
      "155        1    。      O\n",
      "\n",
      "[156 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "test_data = preprocess_data(test_data_path, 155)\n",
    "print(pd.DataFrame(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0  [中, 共, 中, 央, 致, 中, 国, 致, 公, 党, 十, 一, 大, 的, 贺, ...\n",
      "1  [在, 过, 去, 的, 五, 年, 中, ，, 致, 公, 党, 在, 邓, 小, 平, ...\n",
      "                                                   0\n",
      "0  [0, 2, 2, 2, 4, 0, 2, 2, 2, 2, 2, 2, 2, 4, 4, ...\n",
      "1  [4, 4, 4, 4, 4, 4, 4, 4, 0, 2, 2, 4, 1, 3, 3, ...\n"
     ]
    }
   ],
   "source": [
    "def process_data_1():\n",
    "    df = pd.DataFrame(test_data)\n",
    "    enc_tag = preprocessing.LabelEncoder()\n",
    "    df.loc[:, 'tag'] = enc_tag.fit_transform(df['tag'])\n",
    "    sentences = df.groupby('sentence')[\"word\"].apply(list).values\n",
    "    tag = df.groupby('sentence')['tag'].apply(list).values\n",
    "    return sentences, tag, enc_tag\n",
    "\n",
    "sentences, tag, enc_tag = process_data_1()\n",
    "print(pd.DataFrame(sentences))\n",
    "print(pd.DataFrame(tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 101  704 1066  704 1925 5636  704 1744 5636 1062 1054 1282  671 1920\n",
      "  4638 6590 6404 1392  855  807 6134  510 1392  855 1398 2562 8038 1762\n",
      "   704 1744 5636 1062 1054 5018 1282  671 3613 1059 1744  807 6134 1920\n",
      "   833 7384 7028 1374 2458  722 7354 8024  704 1744 1066  772 1054  704\n",
      "  1925 1999 1447  833 6474 1403 1920  833 6134 4850 4178 4164 4638 4867\n",
      "  6590 8024 1403 5636 1062 1054 4638 1398 2562  812 5636  809  779 1147\n",
      "  4638 7309  952 8013  102    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]\n",
      " [ 101 1762 6814 1343 4638  758 2399  704 8024 5636 1062 1054 1762 6924\n",
      "  2207 2398 4415 6389 2900 2471  678 8024 6905 2542 4852  833  712  721\n",
      "  1159 5277 7348 3667 4638 1825 3315 6662 5296 8024 1222 1213 2141 6664\n",
      "  5636 1062 1054 1282 1920 2990 1139 4638 1355 2916 1346 3124 1054 5466\n",
      "  5543  510 1217 2487 5632 6716 2456 6392 4638 1825 3315  818 1218  511\n",
      "   102    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]]\n",
      "[6 0 2 2 2 4 0 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 0 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 4 4 4 4 4 4 4 0 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 0\n",
      " 2 2 4 4 4 4 4 4 4 4 4 4 4 4 6 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7]\n"
     ]
    }
   ],
   "source": [
    "num_tags = 7\n",
    "def create_inputs_targets_1(sentences, tags, tag_encoder):\n",
    "    dataset_dict = {\n",
    "        \"input_ids\": [],\n",
    "        \"token_type_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"tags\": []\n",
    "    }\n",
    "\n",
    "    for sentence, tag in zip(sentences, tags):\n",
    "        input_ids = []\n",
    "        target_tags = []\n",
    "        for idx, word in enumerate(sentence):\n",
    "            ids = tokenizer.encode(word, add_special_tokens=False)\n",
    "            input_ids.extend(ids.ids)\n",
    "            num_tokens = len(ids)\n",
    "            # keep the dim of input_ids and target_tags, e.g. [6] *2 = [6, 6]\n",
    "            target_tags.extend([tag[idx]] * num_tokens)\n",
    "\n",
    "        # Pad truncate, reserve space for[CLS] and [SEP]\n",
    "        input_ids = input_ids[:max_len - 2]\n",
    "        target_tags = target_tags[:max_len - 2]\n",
    "\n",
    "        # https://huggingface.co/docs/transformers/v4.19.2/en/glossary#token-type-ids\n",
    "\n",
    "        input_ids = [101] + input_ids + [102]\n",
    "        target_tags = [num_tags-1] + target_tags + [num_tags-1]  # [num_tags-1] is O\n",
    "        token_type_ids = [0] * len(input_ids) # All is zero means that there is no relation between two sentences\n",
    "        # This argument indicates to the model which tokens should be attended to, and which should not.\n",
    "        attention_mask = [1] * len(input_ids) # The token need to be attended to\n",
    "        padding_len = max_len - len(input_ids)\n",
    "\n",
    "        input_ids = input_ids + ([0] * padding_len) # Add padded tokens\n",
    "        attention_mask = attention_mask + ([0] * padding_len) # Add padded attention mask\n",
    "        token_type_ids = token_type_ids + ([0] * padding_len) # Fill padded token type ids with zero\n",
    "        target_tags = target_tags + ([num_tags] * padding_len) # Fill padded target tags with `undefined` tag\n",
    "\n",
    "        dataset_dict[\"input_ids\"].append(input_ids)\n",
    "        dataset_dict[\"token_type_ids\"].append(token_type_ids)\n",
    "        dataset_dict[\"attention_mask\"].append(attention_mask)\n",
    "        dataset_dict[\"tags\"].append(target_tags)\n",
    "        assert len(\n",
    "            target_tags) == max_len, f'{len(input_ids)}, {len(target_tags)}'\n",
    "\n",
    "    for key in dataset_dict:\n",
    "        dataset_dict[key] = np.array(dataset_dict[key])\n",
    "\n",
    "    x = [\n",
    "        dataset_dict[\"input_ids\"],\n",
    "        dataset_dict[\"token_type_ids\"],\n",
    "        dataset_dict[\"attention_mask\"],\n",
    "    ]\n",
    "    y = dataset_dict[\"tags\"]\n",
    "    return x, y, tag_encoder\n",
    "\n",
    "\n",
    "x, y, tag_encoder = create_inputs_targets_1(sentences, tag, enc_tag)\n",
    "\n",
    "print(x[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inputs_targets(path, max_data_len):\n",
    "    dataset_dict = {\n",
    "        \"input_ids\": [],\n",
    "        \"token_type_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"tags\": []\n",
    "    }\n",
    "    sentences, tags, tag_encoder = process_data(path, max_data_len)\n",
    "\n",
    "    for sentence, tag in zip(sentences, tags):\n",
    "        input_ids = []\n",
    "        target_tags = []\n",
    "        for idx, word in enumerate(sentence):\n",
    "            ids = tokenizer.encode(word, add_special_tokens=False)\n",
    "            input_ids.extend(ids.ids)\n",
    "            num_tokens = len(ids)\n",
    "            # keep the dim of input_ids and target_tags, e.g. [6] *2 = [6, 6]\n",
    "            target_tags.extend([tag[idx]] * num_tokens)\n",
    "\n",
    "        # Pad truncate, reserve space for[CLS] and [SEP]\n",
    "        input_ids = input_ids[:max_len - 2]\n",
    "        target_tags = target_tags[:max_len - 2]\n",
    "\n",
    "        # https://huggingface.co/docs/transformers/v4.19.2/en/glossary#token-type-ids\n",
    "\n",
    "        input_ids = [101] + input_ids + [102]\n",
    "        target_tags = [num_tags-1] + target_tags + [num_tags-1]  # [num_tags-1] is O\n",
    "        token_type_ids = [0] * len(input_ids) # All is zero means that there is no relation between two sentences\n",
    "        # This argument indicates to the model which tokens should be attended to, and which should not.\n",
    "        attention_mask = [1] * len(input_ids) # The token need to be attended to\n",
    "        padding_len = max_len - len(input_ids)\n",
    "\n",
    "        input_ids = input_ids + ([0] * padding_len) # Add padded tokens\n",
    "        attention_mask = attention_mask + ([0] * padding_len) # Add padded attention mask\n",
    "        token_type_ids = token_type_ids + ([0] * padding_len) # Fill padded token type ids with zero\n",
    "        target_tags = target_tags + ([num_tags] * padding_len) # Fill padded target tags with `undefined` tag\n",
    "\n",
    "        dataset_dict[\"input_ids\"].append(input_ids)\n",
    "        dataset_dict[\"token_type_ids\"].append(token_type_ids)\n",
    "        dataset_dict[\"attention_mask\"].append(attention_mask)\n",
    "        dataset_dict[\"tags\"].append(target_tags)\n",
    "        assert len(\n",
    "            target_tags) == max_len, f'{len(input_ids)}, {len(target_tags)}'\n",
    "\n",
    "    for key in dataset_dict:\n",
    "        dataset_dict[key] = np.array(dataset_dict[key])\n",
    "\n",
    "    x = [\n",
    "        dataset_dict[\"input_ids\"],\n",
    "        dataset_dict[\"token_type_ids\"],\n",
    "        dataset_dict[\"attention_mask\"],\n",
    "    ]\n",
    "    y = dataset_dict[\"tags\"]\n",
    "    return x, y, tag_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tags = pd.read_csv(tags_data_path, sep='\\t', names=[\"tag\"])[\"tag\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the slow pretrained tokenizer\n",
    "slow_tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "save_path = \"{}_tokens/\".format(model_name)\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "slow_tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fast tokenizer from saved file\n",
    "tokenizer = BertWordPieceTokenizer(\"{}_tokens/vocab.txt\".format(model_name), lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, tag_encoder = create_inputs_targets(train_data_path, max_data_len=110829) # 221657"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(True, shape=(), dtype=bool)\n",
      "tf.Tensor(False, shape=(), dtype=bool)\n",
      "tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "tf.Tensor(0.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "is_equal = tf.math.equal(7, 7) # True\n",
    "print(is_equal)\n",
    "mask = tf.math.logical_not(is_equal) # False\n",
    "print(mask)\n",
    "mask = tf.cast(mask, dtype=tf.float32) # 0\n",
    "print(mask)\n",
    "loss_ = 0.2\n",
    "loss_ *= mask\n",
    "print(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=False, reduction=tf.keras.losses.Reduction.NONE\n",
    ")\n",
    "\n",
    "# Each batch of data will consist of variable sized sentence tokens with\n",
    "# appropriate padding in both input and target.\n",
    "# During loss calculation, we ignore the loss corresponding padding tokens\n",
    "# in the target.\n",
    "def masked_ce_loss(real, pred):\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    # [pad] -> num_tags(undefined) True -logical_not-> False -cast-> 0 \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, num_tags))\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "\n",
    "def create_model(num_tags):\n",
    "    # BERT encoder\n",
    "    encoder = TFBertModel.from_pretrained(model_name)\n",
    "\n",
    "    # NER Model\n",
    "    # Input() is used to instantiate a Keras tensor.\n",
    "    input_ids = keras.layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "    token_type_ids = keras.layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "    attention_mask = keras.layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "\n",
    "    outputs = encoder(\n",
    "        input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n",
    "    )\n",
    "    # last_hidden_state, more details [here](https://huggingface.co/docs/transformers/v4.19.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions)\n",
    "    embedding = outputs[0]\n",
    "    embedding = keras.layers.Dropout(0.3)(embedding)\n",
    "    tag_logits = keras.layers.Dense(num_tags+1, activation='softmax')(embedding)\n",
    "\n",
    "    model = keras.Model(\n",
    "        inputs=[input_ids, token_type_ids, attention_mask],\n",
    "        outputs=[tag_logits],\n",
    "    )\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    model.compile(optimizer=optimizer, loss=masked_ce_loss, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(num_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 384)]        0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 384)]        0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 384)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  102267648   ['input_1[0][0]',                \n",
      "                                thPoolingAndCrossAt               'input_3[0][0]',                \n",
      "                                tentions(last_hidde               'input_2[0][0]']                \n",
      "                                n_state=(None, 384,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 384, 768)     0           ['tf_bert_model[0][0]']          \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 384, 8)       6152        ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 102,273,800\n",
      "Trainable params: 102,273,800\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=1,\n",
    "    verbose=1,\n",
    "    batch_size=bs,\n",
    "    validation_split=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'ner'\n",
    "saved_model_path = './{}_bert'.format(dataset_name.replace('/', '_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(saved_model_path, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_model = tf.saved_model.load(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_input_from_text(texts, use_tensor=False):\n",
    "    dataset_dict = {\n",
    "        \"input_ids\": [],\n",
    "        \"token_type_ids\": [],\n",
    "        \"attention_mask\": []\n",
    "    }\n",
    "    for sentence in texts:\n",
    "        input_ids = []\n",
    "        for idx, word in enumerate(sentence.split()):\n",
    "            ids = tokenizer.encode(word, add_special_tokens=False)\n",
    "            input_ids.extend(ids.ids)\n",
    "\n",
    "        # Pad and create attention masks.\n",
    "        # Skip if truncation is needed\n",
    "        input_ids = input_ids[:max_len - 2]\n",
    "\n",
    "        input_ids = [101] + input_ids + [102]\n",
    "        n_tokens = len(input_ids)\n",
    "        token_type_ids = [0] * len(input_ids)\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        padding_len = max_len - len(input_ids)\n",
    "\n",
    "        input_ids = input_ids + ([0] * padding_len)\n",
    "        attention_mask = attention_mask + ([0] * padding_len)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_len)\n",
    "\n",
    "        dataset_dict[\"input_ids\"].append(input_ids)\n",
    "        dataset_dict[\"token_type_ids\"].append(token_type_ids)\n",
    "        dataset_dict[\"attention_mask\"].append(attention_mask)\n",
    "\n",
    "    for key in dataset_dict:\n",
    "        dataset_dict[key] = tf.constant(dataset_dict[key]) if use_tensor else np.array(dataset_dict[key])\n",
    "\n",
    "    x = [\n",
    "        dataset_dict[\"input_ids\"],\n",
    "        dataset_dict[\"token_type_ids\"],\n",
    "        dataset_dict[\"attention_mask\"],\n",
    "    ]\n",
    "    return x, n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_text(text, model):\n",
    "    x_test, n_tokens = create_test_input_from_text([text], use_tensor=True)\n",
    "    pred_test = model.predict(x_test) if hasattr(model, 'predict') else model(x_test)\n",
    "    # ignore predictions of padding tokens\n",
    "    pred_tags = np.argmax(pred_test, 2)[0][:n_tokens]\n",
    "    le_dict = dict(zip(tag_encoder.transform(\n",
    "        tag_encoder.classes_), tag_encoder.classes_))\n",
    "    tags = [le_dict.get(_, '[pad]') for _ in pred_tags]\n",
    "    res = []\n",
    "    words = {\n",
    "        'word': '',\n",
    "        'tag': None\n",
    "    }\n",
    "    for idx, tag in enumerate(tags):\n",
    "        token = x_test[0][0][idx]\n",
    "        token = token.numpy()\n",
    "        if(token == 101 or token == 102 or token == None):\n",
    "            continue\n",
    "        if(tag != 'O' and tag != '[pad]'):\n",
    "            pre, suf = tag.split('-')\n",
    "            words['tag'] = suf\n",
    "            word = tokenizer.decode([token])\n",
    "            words['word'] =  words['word'] + word if words['word'] else word\n",
    "        else:\n",
    "            if(words['tag']):\n",
    "                res.append(words)\n",
    "            words = {\n",
    "                'word': '',\n",
    "                'tag': None\n",
    "            }\n",
    "    return pd.DataFrame(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             word  tag\n",
      "0  李华住在朝阳区香河园街道西坝  LOC\n",
      "1            北里社区  LOC\n",
      "2          在5月4号去  LOC\n",
      "3               安  LOC\n",
      "4              广场  LOC\n",
      "5  5号下午去了太阳宫凯德茂商场  LOC\n"
     ]
    }
   ],
   "source": [
    "test_inputs = '李华住在朝阳区香河园街道西坝河北里社区，在5月4号去过天安门广场，5号下午去了太阳宫凯德茂商场。'\n",
    "pre_train_model = model\n",
    "print(predict_from_text(test_inputs, pre_train_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              word  tag\n",
      "0               李华  PER\n",
      "1  朝阳区香河园街道西坝河北里社区  LOC\n",
      "2            天安门广场  LOC\n",
      "3         太阳宫凯德茂商场  LOC\n"
     ]
    }
   ],
   "source": [
    "test_inputs = '李华住在朝阳区香河园街道西坝河北里社区，在5月4号去过天安门广场，5号下午去了太阳宫凯德茂商场。'\n",
    "trained_model = reloaded_model\n",
    "print(predict_from_text(test_inputs, trained_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          word  tag\n",
      "0  通州区张家湾镇环湖小镇  LOC\n",
      "1      东城区东方广场  LOC\n"
     ]
    }
   ],
   "source": [
    "news = '感染者1698：通过主动就诊发现，现住通州区张家湾镇环湖小镇，快递配送员。5月17日曾前往东城区东方广场送快递，自述5月21日、22日先后出现腹泻、发热等症状，5月22日前往医院就诊，5月23日报告核酸检测结果为阳性，当日诊断为确诊病例，临床分型为轻型。'\n",
    "trained_model = reloaded_model\n",
    "print(predict_from_text(news, trained_model))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8639f22e1530ee2a5f2548bc0245ffed892f59880c37de02f243da87cdc45584"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
